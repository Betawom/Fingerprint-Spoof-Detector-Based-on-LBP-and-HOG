{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af59f5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "from skimage.feature import hog,local_binary_pattern\n",
    "import glob\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from skimage import feature\n",
    "from sklearn.metrics import classification_report\n",
    "from skimage.color import rgb2gray\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b4e60d",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe746aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import training data\n",
    "live_train = glob.glob('/Users/rachanatanneeru/Documents/GitHub/Fingerprint-Spoof-Detector-Based-on-LBP-and-HOG/data/Testing Biometrika Live') #positive\n",
    "spoof_train = glob.glob('data/train_spoof/*.png') # negative\n",
    "\n",
    "# import testing data\n",
    "live_test = glob.glob('data/test_live/*.png') # positive\n",
    "spoof_test = glob.glob('data/test_spoof/*.png') # negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98294d43",
   "metadata": {},
   "source": [
    "## Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8f6cf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize empty lists to append data\n",
    "live_train_images = []\n",
    "spoof_train_images = []\n",
    "live_test_images = []\n",
    "spoof_test_images = []\n",
    "\n",
    "# read images from given path and append to the variavles\n",
    "for path in live_train: live_train_images.append(imread(path))\n",
    "for path in spoof_train: spoof_train_images.append(imread(path))\n",
    "for path in live_test: live_test_images.append(imread(path))\n",
    "for path in spoof_test: spoof_test_images.append(imread(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5476edad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert images to np arrays\n",
    "live_train_images, spoof_train_images = np.asarray(live_train_images), np.asarray(spoof_train_images)\n",
    "live_test_images, spoof_test_images = np.asarray(live_test_images), np.asarray(spoof_test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73f5e3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the type of feature you want to use here\n",
    "feature_type = \"lbp\" #hog or lbp\n",
    "\n",
    "# initialize empty lists to append features\n",
    "live_train_features, spoof_train_features = [], []\n",
    "live_test_features, spoof_test_features = [], []\n",
    "\n",
    "if(feature_type == \"hog\"):\n",
    "    \n",
    "    # calculate and store hog features of training data\n",
    "    for live_img, spoof_img in zip(live_train_images, spoof_train_images):\n",
    "        live_train_features.append(hog(live_img, feature_vector = True))\n",
    "        spoof_train_features.append(hog(spoof_img, feature_vector = True))\n",
    "    \n",
    "    # calculate and store hog features of testing data\n",
    "    for live_img, spoof_img in zip(live_test_images, spoof_test_images):\n",
    "        live_test_features.append(hog(live_img, feature_vector = True))\n",
    "        spoof_test_features.append(hog(spoof_img, feature_vector = True))\n",
    "        \n",
    "if(feature_type == \"lbp\"):\n",
    "\n",
    "# parameters copied from scikit docs of lbp\n",
    "    METHOD = 'uniform'\n",
    "    R = 3\n",
    "    P = 8 * R\n",
    "\n",
    "    # calculate and store lbp features of training data\n",
    "    for live_img, spoof_img in zip(live_train_images, spoof_train_images):\n",
    "        live_img = rgb2gray(live_img)\n",
    "        spoof_img = rgb2gray(spoof_img)\n",
    "        live_train_features.append(local_binary_pattern(live_img, P, R, METHOD).flatten())\n",
    "        spoof_train_features.append(local_binary_pattern(spoof_img, P, R, METHOD).flatten())\n",
    "        # calculate and store lbp features of training data   \n",
    "    for live_img, spoof_img in zip(live_test_images, spoof_test_images):\n",
    "        # convert rgb to gray\n",
    "        live_img = rgb2gray(live_img)\n",
    "        # convert rgb to gray\n",
    "        spoof_img = rgb2gray(spoof_img)\n",
    "        live_test_features.append(local_binary_pattern(live_img, P, R, METHOD).flatten())\n",
    "        spoof_test_features.append(local_binary_pattern(spoof_img, P, R, METHOD).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7973904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define labels 1 for live and 0 for spoof\n",
    "# i.e. create a list of 200 1's and 200 0's \n",
    "labels_train = [[1]]*len(live_train_features) + [[0]]*len(spoof_train_features)\n",
    "\n",
    "# merge both positive and negative training data\n",
    "full_train = live_train_features + spoof_train_features\n",
    "\n",
    "# assign labels for all test data, 1 for positive, 0 for negative\n",
    "labels_test = [[1]]*len(live_test_features) + [[0]]*len(spoof_test_features)\n",
    "\n",
    "# merge both positive and negative training data\n",
    "full_test = live_test_features + spoof_test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "206d648c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca1d2dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = np.array(full_train)\n",
    "features_test = np.array(full_test)\n",
    "\n",
    "# stack features and labels together\n",
    "data_frame_train = np.hstack((features_train,labels_train))\n",
    "\n",
    "# shuffle train data\n",
    "np.random.shuffle(data_frame_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e1e2bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and labels of train data\n",
    "x_train = full_train\n",
    "y_train = labels_train\n",
    "\n",
    "# Ready test data and labels\n",
    "x_test = full_test\n",
    "y_test = labels_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e140dbb5",
   "metadata": {},
   "source": [
    "## Normalize/Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97a673a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ms/lq7t8dq95qg5p3vbnfwf9hzh0000gn/T/ipykernel_4099/26128353.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfull_train_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mfull_test_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \"\"\"\n\u001b[1;32m    840\u001b[0m         \u001b[0mfirst_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"n_samples_seen_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;31m# If input is 1D raise error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    770\u001b[0m                     \u001b[0;34m\"Expected 2D array, got 1D array instead:\\narray={}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "full_train_scaled = scaler.fit_transform(x_train)\n",
    "full_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f1d1b5",
   "metadata": {},
   "source": [
    "## Model Building "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad099260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'SVM': SVC(),\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(),\n",
    "    'KNN': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Train each model and store the cross-validation scores\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    # Perform 5-fold cross-validation\n",
    "    cv_scores = cross_val_score(model, full_train_scaled, np.ravel(labels_train), cv=5, scoring='accuracy')\n",
    "    results[name] = cv_scores.mean()\n",
    "\n",
    "# Report cross-validation results in a table\n",
    "print(\"Cross-validation results:\")\n",
    "for name, score in results.items():\n",
    "    print(f\"{name}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8b0e16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183b8f25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ba4df3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
